import requests
from bs4 import BeautifulSoup
import requests
import csv
from urllib.parse import urljoin, urlparse
import time

class Crawler:
    def __init__(self, base_url, delay=1, depth = 3):
        self.base_url = base_url
        self.delay = delay
        self.depth = depth
        self.visited = set()
        self.session = requests.Session()

    def load_page(self, url):
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            return response.text
        except requests.exceptions.RequestException as e:
            print(f"Error {url}: {e}")
            return None
    
    def parsing(self, html, current_url):
        soup = BeautifulSoup(html, 'html.parser')
        links = soup.find_all('a')
        urls = [urljoin(current_url, link.get('href')) for link in links if link.get('href')]
        return urls
    
    def crawler(self, url, depth=0):
        if depth > self.depth or url in self.visited:
            return

        self.visited.add(url)
        html = self.load_page(url)

        if html:
            links = self.parsing(html,url)
            time.sleep(self.delay)

            for link in links:
                parsed_link = urlparse(link)
                if parsed_link.netloc ==urlparse(self.base_url).netloc:
                    self.crawler(link, depth + 1)
if __name__ == "__main__":
    base_url = 'http://example.xxx'
    crawler = Crawler(base_url, depth = 3)
    crawler.crawler(base_url)